# TODO

- why does the model simply output the same token in a loop?
  - something wrong with loss?
- test: argmax -> softmax, debug output problems
- remove ponder entirely, see if that fixes anything
- run a vastly simplified model
